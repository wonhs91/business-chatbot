Metadata-Version: 2.4
Name: business-chatbot-backend
Version: 0.1.0
Summary: LangGraph-powered backend for the AI webchat agent.
Author: Business Chatbot Team
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Requires-Dist: fastapi>=0.121.1
Requires-Dist: uvicorn[standard]>=0.30.0
Requires-Dist: langchain-core>=1.0.4
Requires-Dist: langchain-community>=0.4.1
Requires-Dist: langchain-openai>=1.0.2
Requires-Dist: langgraph>=0.1.21
Requires-Dist: pydantic>=2.12.4
Requires-Dist: python-dotenv>=1.0.1
Requires-Dist: httpx>=0.27.0
Requires-Dist: loguru>=0.7.3
Requires-Dist: tenacity>=9.1.2
Requires-Dist: pyjwt>=2.10.1
Requires-Dist: openai>=2.7.2
Requires-Dist: chromadb<1.4.0,>=1.3.4
Provides-Extra: dev
Requires-Dist: pytest>=9.0.1; extra == "dev"
Requires-Dist: pytest-asyncio>=1.3.0; extra == "dev"
Requires-Dist: httpx>=0.27.0; extra == "dev"
Requires-Dist: ruff>=0.14.4; extra == "dev"
Requires-Dist: jupyter>=1.1.1; extra == "dev"

# Backend Service

This FastAPI service hosts the LangGraph-based conversational agent that powers the website widget.

## Key Modules

- `app/api/`: HTTP and WebSocket routes for chat sessions and health checks.
- `app/agents/`: LangGraph graph definition and supporting nodes.
- `app/services/`: Integrations such as Discord notifications and scheduling APIs.
- `app/retrieval/`: Document ingestion pipeline and vector store utilities.
- `app/models/`: Pydantic schemas shared across modules.
- `app/config/`: Settings management and environment loading.

## Running Locally

```bash
uvicorn app.main:app --reload
```

Before running, create a `.env` file (see `app/config/settings.py`) with credentials for:

- OpenAI (or alternative LLM provider)
- Vector store settings (`CHROMA_PERSIST_DIRECTORY`, `CHROMA_COLLECTION_NAME`)
- Discord webhook URL
- Scheduling provider (Calendly or Google Calendar)
- Optional: LangSmith tracing variables (`LANGSMITH_*`)

### Conversation Memory

The agent uses LangChain's `ChatOpenAI` in stateless mode and layers a server-side
session store (`SessionMemory`) that persists recent chat messages per visitor.
Each request retrieves prior messages, appends the new user prompt, invokes the
model, and updates the history. Swap the in-memory store for Redis or another
shared cache in production to support multi-instance scaling.
